{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/conda_env/torfia/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---------------------------------------------------------------------\n",
    "-- Author: Jhosimar George Arias Figueroa\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Main file to execute the model on the MNIST dataset\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data\n",
    "from model.GMVAE import *\n",
    "\n",
    "#########################################################\n",
    "## Input Parameters\n",
    "#########################################################\n",
    "parser = argparse.ArgumentParser(description='PyTorch Implementation of DGM Clustering')\n",
    "\n",
    "## Used only in notebooks\n",
    "parser.add_argument('-f', '--file',\n",
    "                    help='Path for input file. First line should contain number of lines to search in')\n",
    "\n",
    "parser.add_argument('--expPath', type=str, default=os.path.expanduser('~/pytorchexp/mnistMLPGMVAE'),\n",
    "                    help='Path for exp')\n",
    "\n",
    "## Dataset\n",
    "parser.add_argument('--dataset', type=str, choices=['mnist'],\n",
    "                    default='mnist', help='dataset (default: mnist)')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed (default: 0)')\n",
    "\n",
    "## GPU\n",
    "parser.add_argument('--cuda', type=bool, default=True,\n",
    "                    help='use of cuda (default: False)')\n",
    "parser.add_argument('--gpuID', type=int, default=2,\n",
    "                    help='set gpu id to use (default: 0)')\n",
    "\n",
    "## Training\n",
    "parser.add_argument('--training', type=bool, default=False,\n",
    "                    help='Training phase?')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='number of total epochs to run (default: 200)')\n",
    "parser.add_argument('--batch_size', default=64, type=int,\n",
    "                    help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--batch_size_val', default=200, type=int,\n",
    "                    help='mini-batch size of validation (default: 200)')\n",
    "parser.add_argument('--learning_rate', default=1e-3, type=float,\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--decay_epoch', default=-1, type=int, \n",
    "                    help='Reduces the learning rate every decay_epoch')\n",
    "parser.add_argument('--lr_decay', default=0.5, type=float,\n",
    "                    help='Learning rate decay for training (default: 0.5)')\n",
    "\n",
    "## Architecture\n",
    "parser.add_argument('--num_classes', type=int, default=10,\n",
    "                    help='number of classes (default: 10)')\n",
    "parser.add_argument('--gaussian_size', default=64, type=int,\n",
    "                    help='gaussian size (default: 64)')\n",
    "parser.add_argument('--input_size', default=784, type=int,\n",
    "                    help='input size (default: 784)')\n",
    "\n",
    "## Partition parameters\n",
    "parser.add_argument('--train_proportion', default=1.0, type=float,\n",
    "                    help='proportion of examples to consider for training only (default: 1.0)')\n",
    "\n",
    "## Gumbel parameters\n",
    "parser.add_argument('--init_temp', default=1.0, type=float,\n",
    "                    help='Initial temperature used in gumbel-softmax (recommended 0.5-1.0, default:1.0)')\n",
    "parser.add_argument('--decay_temp', default=1, type=int, \n",
    "                    help='Set 1 to decay gumbel temperature at every epoch (default: 1)')\n",
    "parser.add_argument('--hard_gumbel', default=0, type=int, \n",
    "                    help='Set 1 to use the hard version of gumbel-softmax (default: 1)')\n",
    "parser.add_argument('--min_temp', default=0.5, type=float, \n",
    "                    help='Minimum temperature of gumbel-softmax after annealing (default: 0.5)' )\n",
    "parser.add_argument('--decay_temp_rate', default=0.013862944, type=float,\n",
    "                    help='Temperature decay rate at every epoch (default: 0.013862944)')\n",
    "\n",
    "## Loss function parameters\n",
    "parser.add_argument('--w_gauss', default=1, type=float,\n",
    "                    help='weight of gaussian loss (default: 1)')\n",
    "parser.add_argument('--w_categ', default=1, type=float,\n",
    "                    help='weight of categorical loss (default: 1)')\n",
    "parser.add_argument('--w_rec', default=1, type=float,\n",
    "                    help='weight of reconstruction loss (default: 1)')\n",
    "parser.add_argument('--rec_type', type=str, choices=['bce', 'mse'],\n",
    "                    default='bce', help='desired reconstruction loss function (default: bce)')\n",
    "\n",
    "## Others\n",
    "parser.add_argument('--verbose', default=1, type=int,\n",
    "                    help='print extra information at every epoch.(default: 0)')\n",
    "parser.add_argument('--random_search_it', type=int, default=20,\n",
    "                    help='iterations of random search (default: 20)')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "def ensureDir(path):\n",
    "  # Check whether the specified path exists or not\n",
    "  if not os.path.exists(path):\n",
    "    \n",
    "    # Create a new directory because it does not exist \n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "# Create dir\n",
    "ensureDir(args.expPath)\n",
    "\n",
    "# CUDA Semantics\n",
    "if args.cuda:\n",
    "    args.device = torch.device(\n",
    "        \"cuda:\" + str(args.gpuID) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")\n",
    "\n",
    "# if args.cuda == 1:\n",
    "#    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpuID)\n",
    "\n",
    "## Random Seed\n",
    "SEED = args.seed\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if args.cuda:\n",
    "  torch.cuda.manual_seed(SEED)\n",
    "\n",
    "#########################################################\n",
    "## Read Data\n",
    "#########################################################\n",
    "if args.dataset == \"mnist\":\n",
    "  print(\"Loading mnist dataset...\")\n",
    "  # Download or load downloaded MNIST dataset\n",
    "  train_dataset = datasets.MNIST('./mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "  test_dataset = datasets.MNIST('./mnist', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "#########################################################\n",
    "## Data Partition\n",
    "#########################################################\n",
    "def partition_dataset(n, proportion=0.8):\n",
    "  train_num = int(n * proportion)\n",
    "  indices = np.random.permutation(n)\n",
    "  train_indices, val_indices = indices[:train_num], indices[train_num:]\n",
    "  return train_indices, val_indices\n",
    "\n",
    "if args.train_proportion == 1.0:\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "  val_loader = test_loader\n",
    "else:\n",
    "  train_indices, val_indices = partition_dataset(len(train_dataset), args.train_proportion)\n",
    "  # Create data loaders for train, validation and test datasets\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "  val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size_val, sampler=SubsetRandomSampler(val_indices))\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "\n",
    "\n",
    "#########################################################\n",
    "## Train and Test Model\n",
    "#########################################################\n",
    "gmvae = GMVAE(args)\n",
    "\n",
    "if args.training:\n",
    "  ## Training Phase\n",
    "  history_loss, model, gumbel_temp, hard_gumbel = gmvae.train(train_loader, val_loader)\n",
    "  checkpoint = {\n",
    "                'model': model.inference.state_dict(),\n",
    "                'hard_gumbel': hard_gumbel,\n",
    "                'gumbel_temp' : gumbel_temp\n",
    "            }\n",
    "  torch.save(checkpoint, os.path.join(args.expPath,'inference-mlp-original.pt'))\n",
    "\n",
    "  ## Testing Phase\n",
    "  accuracy, nmi = gmvae.test(test_loader)\n",
    "  print(\"Testing phase...\")\n",
    "  print(\"Accuracy: %.5lf, NMI: %.5lf\" % (accuracy, nmi) )\n",
    "else:\n",
    "  # Create model\n",
    "  loaded_inference_model = gmvae.network.inference\n",
    "\n",
    "  # load checkpoint and parameters\n",
    "  loaded = torch.load(os.path.join(args.expPath,'inference-mlp-original.pt'))\n",
    "  m_state_dict = loaded['model']\n",
    "  hard_gumbel = loaded['hard_gumbel']\n",
    "  gumbel_temp = loaded['gumbel_temp']\n",
    "\n",
    "  # update model\n",
    "  loaded_inference_model.load_state_dict(m_state_dict)\n",
    "  \n",
    "  features = []\n",
    "  labels = []\n",
    "  with torch.no_grad():\n",
    "      for data, label in test_loader:\n",
    "        data = data.to(args.device)\n",
    "\n",
    "        # size_int = 128\n",
    "        # data = F.interpolate(data, size=size_int)\n",
    "        # data = data.repeat(1, 3, 1, 1)  # Grayscale to RGB!\n",
    "      \n",
    "        # flatten data\n",
    "        data = data.view(data.size(0), -1)\n",
    "\n",
    "        # forward call\n",
    "        out_net = loaded_inference_model(data, gumbel_temp, hard_gumbel)\n",
    "        \n",
    "        # Feature extraction for batch\n",
    "        feature_batch = out_net['gaussian'].detach().cpu().numpy()\n",
    "        features.append(feature_batch)\n",
    "        labels.append(label[:,np.newaxis])\n",
    "\n",
    "  # Ultimate features\n",
    "  features = np.vstack(features)\n",
    "  labels = np.vstack(labels)\n",
    "  data_to_save = {'features': features, 'labels': labels}\n",
    "\n",
    "  with open(os.path.join(args.expPath,'features.pickle'), 'wb') as handle:\n",
    "    pickle.dump(data_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "  # Sanity check\n",
    "  with open(os.path.join(args.expPath,'features.pickle'), 'rb') as handle:\n",
    "    data_to_load = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # TSNE\n",
    "  import numpy as np\n",
    "  from sklearn.manifold import TSNE\n",
    "\n",
    "  tsne_kwargs = {\n",
    "        \"n_components\": 2,\n",
    "        \"n_iter\": 1000,\n",
    "        \"n_iter_without_progress\": 200,\n",
    "        \"random_state\": 9,\n",
    "    }\n",
    "\n",
    "  tsne_model = TSNE(**tsne_kwargs)\n",
    "  res = tsne_model.fit_transform(data_to_load['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(35.347222222222214, 0.5, 'tSNE Dimension 2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLOTTING FOR ENTIRE DATASET\n",
    "plt.scatter(res[:, 0], res[:, 1], marker='o')\n",
    "plt.xlabel('tSNE Dimension 1')\n",
    "plt.ylabel('tSNE Dimension 2')\n",
    "plt.show()\n",
    "# plt.savefig(os.path.join(args.expPath,'tSNE_2components_full.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(35.347222222222214, 0.5, 'tSNE Dimension 2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOMLY SAMPLE 500 POINTS FOR PLOTTING\n",
    "idx = np.random.randint(res.shape[0], size=500)\n",
    "plt.scatter(res[idx, 0], res[idx, 1], marker='o')\n",
    "plt.xlabel('tSNE Dimension 1')\n",
    "plt.ylabel('tSNE Dimension 2')\n",
    "# plt.savefig(os.path.join(args.expPath,'tSNE_2components_sample500.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b0486a6c7064407abb9a91c579bba5362514dc34400476a34e10dc3dda40b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('torfia': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
